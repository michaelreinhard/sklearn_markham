{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the imports. Packages to import are \n",
    "> pandas  \n",
    "> OneHotEncoder  \n",
    "> LogisticRegression  \n",
    "> MakeColumnTransformer  \n",
    "> make_pipeline \n",
    "> CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the list of columns we want in the model, the 'Parch' (the number of parents or siblings they had on the trip), the amount they paid to get on the boat, the port they got on the boat at, and thier gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch', 'Fare', 'Embarked', 'Sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into a Pandas DateFrame from the website abbreviated version of the kaggle website. Import the \\\n",
    "training set. Then assign the columns and the target variable to X and y, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://bit.ly/kaggletrain'\n",
    "df = pd.read_csv(url, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cols]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and assign the testing set to the X_new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_test = 'http://bit.ly/kaggletest'\n",
    "df_new = pd.read_csv(url_test, nrows=10)\n",
    "X_new = df_new[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['C', 'Q', 'S'], dtype=object), array(['female', 'male'], dtype=object)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(df[['Embarked','Sex']])\n",
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct and instatiate the column transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (ohe, ['Embarked', 'Sex']),\n",
    "        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the column transformer to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  7.25  ],\n",
       "       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 71.2833],\n",
       "       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    ,  7.925 ],\n",
       "       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  0.    , 53.1   ],\n",
       "       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    ,  8.05  ],\n",
       "       [ 0.    ,  1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  8.4583],\n",
       "       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  0.    , 51.8625],\n",
       "       [ 0.    ,  0.    ,  1.    ,  0.    ,  1.    ,  1.    , 21.075 ],\n",
       "       [ 0.    ,  0.    ,  1.    ,  1.    ,  0.    ,  2.    , 11.1333],\n",
       "       [ 1.    ,  0.    ,  0.    ,  1.    ,  0.    ,  0.    , 30.0708]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the logistic Regression model and assign a solver for small data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pipeline and the fit the new model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('onehotencoder',\n",
       "                                                  OneHotEncoder(categorical_features=None,\n",
       "                                                                categories=None,\n",
       "                                                                drop=None,\n",
       "                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                handle_unknown='error',\n",
       "                                                                n_values=None,\n",
       "                                                                sparse=True),\n",
       "                                                  ['Embarked', 'Sex'])],\n",
       "                                   verbose=False)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=1,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26491287, -0.19848033, -0.22907928,  1.0075062 , -1.17015293,\n",
       "         0.20056557,  0.01597307]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.logisticregression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use text data. First immport the text tranformer module, instantiate it, and vectorize the variable 'Name' into a document-test matrix. Remember that CountVectorizer expects one-dimensional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "dtm = vect.fit_transform(df['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achem', 'adele', 'allen', 'berg', 'bradley', 'braund', 'briggs', 'cumings', 'elisabeth', 'florence', 'futrelle', 'gosta', 'harris', 'heath', 'heikkinen', 'henry', 'jacques', 'james', 'john', 'johnson', 'laina', 'leonard', 'lily', 'master', 'may', 'mccarthy', 'miss', 'moran', 'mr', 'mrs', 'nasser', 'nicholas', 'oscar', 'owen', 'palsson', 'peel', 'thayer', 'timothy', 'vilhelmina', 'william']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>achem</th>\n",
       "      <th>adele</th>\n",
       "      <th>allen</th>\n",
       "      <th>berg</th>\n",
       "      <th>bradley</th>\n",
       "      <th>braund</th>\n",
       "      <th>briggs</th>\n",
       "      <th>cumings</th>\n",
       "      <th>elisabeth</th>\n",
       "      <th>florence</th>\n",
       "      <th>...</th>\n",
       "      <th>nasser</th>\n",
       "      <th>nicholas</th>\n",
       "      <th>oscar</th>\n",
       "      <th>owen</th>\n",
       "      <th>palsson</th>\n",
       "      <th>peel</th>\n",
       "      <th>thayer</th>\n",
       "      <th>timothy</th>\n",
       "      <th>vilhelmina</th>\n",
       "      <th>william</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   achem  adele  allen  berg  bradley  braund  briggs  cumings  elisabeth  \\\n",
       "0      0      0      0     0        0       1       0        0          0   \n",
       "1      0      0      0     0        1       0       1        1          0   \n",
       "2      0      0      0     0        0       0       0        0          0   \n",
       "3      0      0      0     0        0       0       0        0          0   \n",
       "4      0      0      1     0        0       0       0        0          0   \n",
       "5      0      0      0     0        0       0       0        0          0   \n",
       "6      0      0      0     0        0       0       0        0          0   \n",
       "7      0      0      0     0        0       0       0        0          0   \n",
       "8      0      0      0     1        0       0       0        0          1   \n",
       "9      1      1      0     0        0       0       0        0          0   \n",
       "\n",
       "   florence  ...  nasser  nicholas  oscar  owen  palsson  peel  thayer  \\\n",
       "0         0  ...       0         0      0     1        0     0       0   \n",
       "1         1  ...       0         0      0     0        0     0       1   \n",
       "2         0  ...       0         0      0     0        0     0       0   \n",
       "3         0  ...       0         0      0     0        0     1       0   \n",
       "4         0  ...       0         0      0     0        0     0       0   \n",
       "5         0  ...       0         0      0     0        0     0       0   \n",
       "6         0  ...       0         0      0     0        0     0       0   \n",
       "7         0  ...       0         0      0     0        1     0       0   \n",
       "8         0  ...       0         0      1     0        0     0       0   \n",
       "9         0  ...       1         1      0     0        0     0       0   \n",
       "\n",
       "   timothy  vilhelmina  william  \n",
       "0        0           0        0  \n",
       "1        0           0        0  \n",
       "2        0           0        0  \n",
       "3        0           0        0  \n",
       "4        0           0        1  \n",
       "5        0           0        0  \n",
       "6        1           0        0  \n",
       "7        0           0        0  \n",
       "8        0           1        0  \n",
       "9        0           0        0  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update X to include 'Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Parch','Fare','Embarked','Sex','Name']\n",
    "X = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the ColumnTransformer without the passthrough to examine in a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (ohe, ['Embarked','Sex']),\n",
    "        (vect, 'Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehotencoder__x0_C</th>\n",
       "      <th>onehotencoder__x0_Q</th>\n",
       "      <th>onehotencoder__x0_S</th>\n",
       "      <th>onehotencoder__x1_female</th>\n",
       "      <th>onehotencoder__x1_male</th>\n",
       "      <th>countvectorizer__achem</th>\n",
       "      <th>countvectorizer__adele</th>\n",
       "      <th>countvectorizer__allen</th>\n",
       "      <th>countvectorizer__berg</th>\n",
       "      <th>countvectorizer__bradley</th>\n",
       "      <th>...</th>\n",
       "      <th>countvectorizer__nasser</th>\n",
       "      <th>countvectorizer__nicholas</th>\n",
       "      <th>countvectorizer__oscar</th>\n",
       "      <th>countvectorizer__owen</th>\n",
       "      <th>countvectorizer__palsson</th>\n",
       "      <th>countvectorizer__peel</th>\n",
       "      <th>countvectorizer__thayer</th>\n",
       "      <th>countvectorizer__timothy</th>\n",
       "      <th>countvectorizer__vilhelmina</th>\n",
       "      <th>countvectorizer__william</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   onehotencoder__x0_C  onehotencoder__x0_Q  onehotencoder__x0_S  \\\n",
       "0                  0.0                  0.0                  1.0   \n",
       "1                  1.0                  0.0                  0.0   \n",
       "2                  0.0                  0.0                  1.0   \n",
       "3                  0.0                  0.0                  1.0   \n",
       "4                  0.0                  0.0                  1.0   \n",
       "5                  0.0                  1.0                  0.0   \n",
       "6                  0.0                  0.0                  1.0   \n",
       "7                  0.0                  0.0                  1.0   \n",
       "8                  0.0                  0.0                  1.0   \n",
       "9                  1.0                  0.0                  0.0   \n",
       "\n",
       "   onehotencoder__x1_female  onehotencoder__x1_male  countvectorizer__achem  \\\n",
       "0                       0.0                     1.0                     0.0   \n",
       "1                       1.0                     0.0                     0.0   \n",
       "2                       1.0                     0.0                     0.0   \n",
       "3                       1.0                     0.0                     0.0   \n",
       "4                       0.0                     1.0                     0.0   \n",
       "5                       0.0                     1.0                     0.0   \n",
       "6                       0.0                     1.0                     0.0   \n",
       "7                       0.0                     1.0                     0.0   \n",
       "8                       1.0                     0.0                     0.0   \n",
       "9                       1.0                     0.0                     1.0   \n",
       "\n",
       "   countvectorizer__adele  countvectorizer__allen  countvectorizer__berg  \\\n",
       "0                     0.0                     0.0                    0.0   \n",
       "1                     0.0                     0.0                    0.0   \n",
       "2                     0.0                     0.0                    0.0   \n",
       "3                     0.0                     0.0                    0.0   \n",
       "4                     0.0                     1.0                    0.0   \n",
       "5                     0.0                     0.0                    0.0   \n",
       "6                     0.0                     0.0                    0.0   \n",
       "7                     0.0                     0.0                    0.0   \n",
       "8                     0.0                     0.0                    1.0   \n",
       "9                     1.0                     0.0                    0.0   \n",
       "\n",
       "   countvectorizer__bradley  ...  countvectorizer__nasser  \\\n",
       "0                       0.0  ...                      0.0   \n",
       "1                       1.0  ...                      0.0   \n",
       "2                       0.0  ...                      0.0   \n",
       "3                       0.0  ...                      0.0   \n",
       "4                       0.0  ...                      0.0   \n",
       "5                       0.0  ...                      0.0   \n",
       "6                       0.0  ...                      0.0   \n",
       "7                       0.0  ...                      0.0   \n",
       "8                       0.0  ...                      0.0   \n",
       "9                       0.0  ...                      1.0   \n",
       "\n",
       "   countvectorizer__nicholas  countvectorizer__oscar  countvectorizer__owen  \\\n",
       "0                        0.0                     0.0                    1.0   \n",
       "1                        0.0                     0.0                    0.0   \n",
       "2                        0.0                     0.0                    0.0   \n",
       "3                        0.0                     0.0                    0.0   \n",
       "4                        0.0                     0.0                    0.0   \n",
       "5                        0.0                     0.0                    0.0   \n",
       "6                        0.0                     0.0                    0.0   \n",
       "7                        0.0                     0.0                    0.0   \n",
       "8                        0.0                     1.0                    0.0   \n",
       "9                        1.0                     0.0                    0.0   \n",
       "\n",
       "   countvectorizer__palsson  countvectorizer__peel  countvectorizer__thayer  \\\n",
       "0                       0.0                    0.0                      0.0   \n",
       "1                       0.0                    0.0                      1.0   \n",
       "2                       0.0                    0.0                      0.0   \n",
       "3                       0.0                    1.0                      0.0   \n",
       "4                       0.0                    0.0                      0.0   \n",
       "5                       0.0                    0.0                      0.0   \n",
       "6                       0.0                    0.0                      0.0   \n",
       "7                       1.0                    0.0                      0.0   \n",
       "8                       0.0                    0.0                      0.0   \n",
       "9                       0.0                    0.0                      0.0   \n",
       "\n",
       "   countvectorizer__timothy  countvectorizer__vilhelmina  \\\n",
       "0                       0.0                          0.0   \n",
       "1                       0.0                          0.0   \n",
       "2                       0.0                          0.0   \n",
       "3                       0.0                          0.0   \n",
       "4                       0.0                          0.0   \n",
       "5                       0.0                          0.0   \n",
       "6                       1.0                          0.0   \n",
       "7                       0.0                          0.0   \n",
       "8                       0.0                          1.0   \n",
       "9                       0.0                          0.0   \n",
       "\n",
       "   countvectorizer__william  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       1.0  \n",
       "5                       0.0  \n",
       "6                       0.0  \n",
       "7                       0.0  \n",
       "8                       0.0  \n",
       "9                       0.0  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=ct.fit_transform(X).toarray(), columns=ct.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make column transformer and add in the *passthrough* statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (ohe, ['Embarked','Sex']),\n",
    "        (vect, 'Name'),\n",
    "         remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x47 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 78 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, update the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline and examine the steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y);\n",
    "# pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the testing data frame to include the new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>Q</td>\n",
       "      <td>male</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>Q</td>\n",
       "      <td>male</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9.2250</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Svensson, Mr. Johan Cervin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>Q</td>\n",
       "      <td>female</td>\n",
       "      <td>Connolly, Miss. Kate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Caldwell, Mr. Albert Francis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>C</td>\n",
       "      <td>female</td>\n",
       "      <td>Abrahim, Mrs. Joseph (Sophie Halaut Easu)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>S</td>\n",
       "      <td>male</td>\n",
       "      <td>Davies, Mr. John Samuel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Parch     Fare Embarked     Sex  \\\n",
       "0      0   7.8292        Q    male   \n",
       "1      0   7.0000        S  female   \n",
       "2      0   9.6875        Q    male   \n",
       "3      0   8.6625        S    male   \n",
       "4      1  12.2875        S  female   \n",
       "5      0   9.2250        S    male   \n",
       "6      0   7.6292        Q  female   \n",
       "7      1  29.0000        S    male   \n",
       "8      0   7.2292        C  female   \n",
       "9      0  24.1500        S    male   \n",
       "\n",
       "                                           Name  \n",
       "0                              Kelly, Mr. James  \n",
       "1              Wilkes, Mrs. James (Ellen Needs)  \n",
       "2                     Myles, Mr. Thomas Francis  \n",
       "3                              Wirz, Mr. Albert  \n",
       "4  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  \n",
       "5                    Svensson, Mr. Johan Cervin  \n",
       "6                          Connolly, Miss. Kate  \n",
       "7                  Caldwell, Mr. Albert Francis  \n",
       "8     Abrahim, Mrs. Joseph (Sophie Halaut Easu)  \n",
       "9                       Davies, Mr. John Samuel  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = df_new[cols]\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding age. Introduces missing values problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.append('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throws error: ValueError: Input contains NaN, infinity or a value too large for dtype('float64')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:  \n",
    "dropna() -- This is good if there are only a small number of missing and you know that the process that lead to the missing data is totally random. If you do this do it before peeling off your X so you get the y's  \n",
    "\n",
    "dropna(axis='columns') drops the variable, the whole column. So in this case it would drop 'Age'.  \n",
    "\n",
    "The alternative is to impute missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the package that imputes values, instantiate it, and run it on the variable 'Age'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.        ],\n",
       "       [38.        ],\n",
       "       [26.        ],\n",
       "       [35.        ],\n",
       "       [35.        ],\n",
       "       [28.11111111],\n",
       "       [54.        ],\n",
       "       [ 2.        ],\n",
       "       [27.        ],\n",
       "       [14.        ]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer()\n",
    "imp.fit_transform(X[['Age']]) #used default mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out the value it used as the imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.11111111])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will replace any NaN with this, the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (ohe, ['Embarked','Sex']),\n",
    "        (vect, 'Name'),\n",
    "        (imp, ['Age']),\n",
    "         remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('onehotencoder',\n",
       "                                                  OneHotEncoder(categorical_features=None,\n",
       "                                                                categories=None,\n",
       "                                                                drop=None,\n",
       "                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                handle_unknown='error',\n",
       "                                                                n_values=None,\n",
       "                                                                sparse=True),\n",
       "                                                  ['Embarked', 'Sex']),\n",
       "                                                 ('cou...\n",
       "                                                                fill_value=None,\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='mean',\n",
       "                                                                verbose=0),\n",
       "                                                  ['Age'])],\n",
       "                                   verbose=False)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=1,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer': ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
       "                   transformer_weights=None,\n",
       "                   transformers=[('onehotencoder',\n",
       "                                  OneHotEncoder(categorical_features=None,\n",
       "                                                categories=None, drop=None,\n",
       "                                                dtype=<class 'numpy.float64'>,\n",
       "                                                handle_unknown='error',\n",
       "                                                n_values=None, sparse=True),\n",
       "                                  ['Embarked', 'Sex']),\n",
       "                                 ('countvectorizer',\n",
       "                                  CountVectorizer(analyzer='word', bin...\n",
       "                                                  input='content',\n",
       "                                                  lowercase=True, max_df=1.0,\n",
       "                                                  max_features=None, min_df=1,\n",
       "                                                  ngram_range=(1, 1),\n",
       "                                                  preprocessor=None,\n",
       "                                                  stop_words=None,\n",
       "                                                  strip_accents=None,\n",
       "                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                  tokenizer=None,\n",
       "                                                  vocabulary=None),\n",
       "                                  'Name'),\n",
       "                                 ('simpleimputer',\n",
       "                                  SimpleImputer(add_indicator=False, copy=True,\n",
       "                                                fill_value=None,\n",
       "                                                missing_values=nan,\n",
       "                                                strategy='mean', verbose=0),\n",
       "                                  ['Age'])],\n",
       "                   verbose=False),\n",
       " 'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=1, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False)}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now modify the test data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = df_new[cols]\n",
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X_new has missing values, the value that will be imputed is the mean of age in X, not in X_new. It will fill in the value discovered from the training data, not the testing data. We are only allowed to learn from training data, not the testing data. This is analogous to the vectorizing and dummy values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try a new imputer that in addition to imputing values to missing data, creates a binary variable indicating whether the value is imputed. This can be used as a new variable on the theory that the process which lead to data being missing was non-random and causally relevant to the process under investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_indicator = SimpleImputer(add_indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.        ,  0.        ],\n",
       "       [38.        ,  0.        ],\n",
       "       [26.        ,  0.        ],\n",
       "       [35.        ,  0.        ],\n",
       "       [35.        ,  0.        ],\n",
       "       [28.11111111,  1.        ],\n",
       "       [54.        ,  0.        ],\n",
       "       [ 2.        ,  0.        ],\n",
       "       [27.        ,  0.        ],\n",
       "       [14.        ,  0.        ]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_indicator.fit_transform(X[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 1 where the value was imputed. This is useful when you think that having 'missingness' is a predictive feature itself. 'Missing not at random', MNAR, is common and my be useful to use as a variable. There is knn imputer and other imputers but they are more complex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to the full data set to introduce more NaN problems. Import the whole data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://bit.ly/kaggletrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('http://bit.ly/kaggletest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has only 11 columns because it doesn't have the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count up the number of values that are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embarked has missing values in the training set but not in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fare` has missing values in the testing set but not in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the X and y variables with the new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[cols]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the column transformer with the new variable, Age, in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (ohe, ['Embarked','Sex']),\n",
    "        (vect, 'Name'),\n",
    "        (imp, ['Age']),\n",
    "         remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct.fit_transform(X, y)  Fails because Embarked has NaN positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code fails because there are missing values in the Embarked variable. So first imput a constant. There is no way to 'average' the values of a categorical variable so we have to impute a constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_constant = SimpleImputer(strategy='constant', fill_value='missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is imputing values. It is just imputing the value 'missing', not trying to guess what the real value would have been. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new pipeline that only imputes missing values and then creates the dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ohe = make_pipeline(imp_constant, ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have made a pipeling that only has imputation and one_hot_encoding, a transformer only pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<891x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 891 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_ohe.fit_transform(X[['Embarked']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses constant imputation on Embarked and then passes the result to the one-hot-encoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule for transformer: all pipeline steps except the final step must be transformers. The last step can be either a transformer or a model.  \n",
    "\n",
    "Now we modify the pipeline to have the ohe be an imp_ohe. Embarked has missing values but Sex does not. But even though the Sex variable does not have any missing values in the training set it has some in the texting set. Passing it to the `Sex` variable to the one_hot_encoder will make it be coded when the missing values are encountered in the testing data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (imp_ohe, ['Embarked','Sex']),\n",
    "        (vect, 'Name'),\n",
    "        (imp, ['Age']),\n",
    "         remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<891x1518 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7328 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second problem we needed to solve is that 'Fare' has missing values in test but did not have them in train. That is an easy problem to fix. We just add 'Fare' into the imputer and the imputer learns an imputation value from the training data, even though it will only use them on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "        (imp_ohe, ['Embarked','Sex']),\n",
    "        (vect, 'Name'),\n",
    "        (imp, ['Age', 'Fare']),\n",
    "         remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<891x1518 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7328 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, logreg)\n",
    "pipe.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = df_new[cols]\n",
    "pipe.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.69911765, 32.20420797])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imputed values for age and fare\n",
    "ct.named_transformers_.simpleimputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the average age of the people in the data set was 29.7 and the average fare was 32.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap at http://bit.ly/complex-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why wouldn't we do all these data transformations in Pandas and save sklearn for the modeling phase. There are 3 drawbacks.  \n",
    "\n",
    "1) you can't use CountVectorizer with Pandas.  \n",
    "\n",
    "2) you could do the one-hot-encoding in pandas and add them back in when you get into sklearn.  \n",
    "\n",
    "3) If you do the dummy variables in Pandas then you are going to have 'data leakage' in the next stage when you split into X_train and X_test. Your model evaluation procedure will not be a simulation of reality. Your imputation values will be learned from the entire data set. You will have learned from what is eventually be the testing set. Your model has to learn only from the training set. This will happen with all kinds of transformations if you do the transformations in Pandas. \n",
    "\n",
    "Data leakage: learning from the testing data that you are not allowed to know.  \n",
    "\n",
    "sklearn prevents that by having separate steps for all the transformers that can operate on just the training data.  \n",
    "\n",
    "Aren't there column transformations that you can't easily do in sklearn. Like multiplying two columns together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-evaluating the model with cross-validation. We should have been doing this all through but we only had 5 cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8114787748152608"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running cross val score on the pipeline it splits the data leaving 4/5 to the transformations, training and estimating the model and then do another split. It always does the transformations after the split to avoid data leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to tune the logistic regression hyper parameters and the transformations. To do this we use grid-search. 'hyper-parameters' are set by you, 'parameters' are learned by the model. Since we are tuning a pipeline we need the parameter names. We have to have the parameter names that are used in the pipeline so we get them from the pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['columntransformer', 'logisticregression'])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a dictionary called 'params' and add as the values the values we want to try for that parameter. We will tune the 'penalty' and the C parameter for the logistic regression. The penalty takes the values l1 (the letter 'l' and the number 1) and l2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['logisticregression__penalty'] = ['l1', 'l2']\n",
    "params['logisticregression__C'] = [0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the grid search function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it looks like cross_val_score. We specify the pipe, the parameters, the number of 'folds' we want it to cross-validata, and the scoring method for evaluating the models. Set up the grid-search object and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in a data frame and sort the values according to the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <th>param_logisticregression__penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'logisticregression__C': 10, 'logisticregress...</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.019263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023507</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'logisticregression__C': 1, 'logisticregressi...</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.821229</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'logisticregression__C': 1, 'logisticregressi...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.019987</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025153</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'logisticregression__C': 10, 'logisticregress...</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.809203</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.021262</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.008359</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.807910</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.023885</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.782267</td>\n",
       "      <td>0.018048</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4       0.032313      0.006308         0.008949        0.001055   \n",
       "2       0.023507      0.000808         0.008263        0.000247   \n",
       "3       0.022361      0.000882         0.008090        0.000173   \n",
       "5       0.025153      0.004034         0.013276        0.009856   \n",
       "1       0.021262      0.000266         0.008359        0.000336   \n",
       "0       0.023885      0.003320         0.010132        0.002394   \n",
       "\n",
       "  param_logisticregression__C param_logisticregression__penalty  \\\n",
       "4                          10                                l1   \n",
       "2                           1                                l1   \n",
       "3                           1                                l2   \n",
       "5                          10                                l2   \n",
       "1                         0.1                                l2   \n",
       "0                         0.1                                l1   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "4  {'logisticregression__C': 10, 'logisticregress...           0.821229   \n",
       "2  {'logisticregression__C': 1, 'logisticregressi...           0.815642   \n",
       "3  {'logisticregression__C': 1, 'logisticregressi...           0.798883   \n",
       "5  {'logisticregression__C': 10, 'logisticregress...           0.782123   \n",
       "1  {'logisticregression__C': 0.1, 'logisticregres...           0.798883   \n",
       "0  {'logisticregression__C': 0.1, 'logisticregres...           0.787709   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "4           0.821229           0.820225           0.792135           0.853107   \n",
       "2           0.821229           0.797753           0.792135           0.847458   \n",
       "3           0.826816           0.803371           0.786517           0.841808   \n",
       "5           0.804469           0.808989           0.797753           0.853107   \n",
       "1           0.804469           0.764045           0.775281           0.807910   \n",
       "0           0.804469           0.764045           0.758427           0.796610   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "4         0.821549        0.019263                1  \n",
       "2         0.814815        0.019519                2  \n",
       "3         0.811448        0.019987                3  \n",
       "5         0.809203        0.023684                4  \n",
       "1         0.790123        0.017305                5  \n",
       "0         0.782267        0.018048                6  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(add_indicator=False,\n",
       "                                                                copy=True,\n",
       "                                                                fill_value='missing',\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='constant',\n",
       "                                                                verbose=0)),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(categorical_features=None,\n",
       "                                                                catego...\n",
       "                                                 lowercase=True, max_df=1.0,\n",
       "                                                 max_features=None, min_df=1,\n",
       "                                                 ngram_range=(1, 1),\n",
       "                                                 preprocessor=None,\n",
       "                                                 stop_words=None,\n",
       "                                                 strip_accents=None,\n",
       "                                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                 tokenizer=None,\n",
       "                                                 vocabulary=None),\n",
       "                                 'Name'),\n",
       "                                ('simpleimputer',\n",
       "                                 SimpleImputer(add_indicator=False, copy=True,\n",
       "                                               fill_value=None,\n",
       "                                               missing_values=nan,\n",
       "                                               strategy='mean', verbose=0),\n",
       "                                 ['Age', 'Fare'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.columntransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding `.named_transformers_` provides a more compact output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline': Pipeline(memory=None,\n",
       "          steps=[('simpleimputer',\n",
       "                  SimpleImputer(add_indicator=False, copy=True,\n",
       "                                fill_value='missing', missing_values=nan,\n",
       "                                strategy='constant', verbose=0)),\n",
       "                 ('onehotencoder',\n",
       "                  OneHotEncoder(categorical_features=None, categories=None,\n",
       "                                drop=None, dtype=<class 'numpy.float64'>,\n",
       "                                handle_unknown='error', n_values=None,\n",
       "                                sparse=True))],\n",
       "          verbose=False),\n",
       " 'countvectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'simpleimputer': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='mean', verbose=0),\n",
       " 'remainder': 'passthrough'}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.columntransformer.named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With making a dummy variable you have the option to drop the first category. markham recommends not doing it because 'reasons'. But here in the parameters we want to send to grid_search, we want to try both to see if it makes any difference. So we run the model once with all the categories for the dummy variables and once dropping the first category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__pipeline__onehotencoder__drop'] = [None, 'first']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether n_grams make a difference by specifying the model with tokenizing words or pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__countvectorizer__ngram_range'] = [(1,1), (1,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether adding an indicator of having imputed a value or not makes a difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['columntransformer__simpleimputer__add_indicator'] = [False, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the final product by printing out the params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__penalty': ['l1', 'l2'],\n",
       " 'logisticregression__C': [0.1, 1, 10],\n",
       " 'columntransformer__pipeline__onehotencoder__drop': [None, 'first'],\n",
       " 'columntransformer__countvectorizer__ngram_range': [(1, 1), (1, 2)],\n",
       " 'columntransformer__simpleimputer__add_indicator': [False, False]}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make this more elegant by giving the steps custom names by using 'Pipeline' instead of 'make_pipeline' and 'ColumnTransformer' instead of 'make_column_transformer'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('columntransformer',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='passthrough',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('pipeline',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(add_indicator=False,\n",
       "                                                                                                        copy=True,\n",
       "                                                                                                        fill_value='missing',\n",
       "                                                                                                        missi...\n",
       "             param_grid={'columntransformer__countvectorizer__ngram_range': [(1,\n",
       "                                                                              1),\n",
       "                                                                             (1,\n",
       "                                                                              2)],\n",
       "                         'columntransformer__pipeline__onehotencoder__drop': [None,\n",
       "                                                                              'first'],\n",
       "                         'columntransformer__simpleimputer__add_indicator': [False,\n",
       "                                                                             False],\n",
       "                         'logisticregression__C': [0.1, 1, 10],\n",
       "                         'logisticregression__penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8282828282828283"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer__countvectorizer__ngram_range': (1, 2),\n",
       " 'columntransformer__pipeline__onehotencoder__drop': None,\n",
       " 'columntransformer__simpleimputer__add_indicator': False,\n",
       " 'logisticregression__C': 10,\n",
       " 'logisticregression__penalty': 'l1'}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid automatically choses the best parameters and uses those for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The double underscore signals ? Because we need to distinguish between pipeline steps and parameter names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
